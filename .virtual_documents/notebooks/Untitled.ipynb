import pandas as pd
import os

DATA_DIR = os.path.join("..", "data", "raw")

median_income_df = pd.read_csv(os.path.join(DATA_DIR, "Household_Income.csv"))
median_income_df.head()
cdta_df = pd.read_csv(os.path.join(DATA_DIR, "2020_Community_District_Tabulation.csv"))
cdta_df.sample(5)


# Quick check
# Check columns
print("House Hold Income Columns:\n", median_income_df.columns, "\n")
print("CDTA Columns:\n", cdta_df.columns, "\n")
median_income_df.sample(5)


# Get the size of the dataframe
rows, cols = cdta_df.shape
print(f"Merged DataFrame size: {rows} rows x {cols} columns")


# Step 1: Extract the code in parentheses
median_income_df['Code'] = median_income_df['Location'].str.extract(r'\((.*?)\)')
median_income_df.head()


# Step 2: Replace letters only (prefix replacements)
letter_map = {
    "K": "BK",
   
    "B": "BX",
    "M": "MN",
    
}






# Function to replace letters while keeping numbers
def replace_prefix(code):
    if pd.isna(code):
        return code
    for old, new in letter_map.items():
        if code.startswith(old):
            return new + code[len(old):]
    return code  # leave as-is if no match

median_income_df['Code'] = median_income_df['Code'].apply(replace_prefix)

# Optional: check first few rows
print(median_income_df.sample(5))




# Get the size of the dataframe
rows, cols = median_income_df.shape
print(f"Merged DataFrame size: {rows} rows x {cols} columns")

#Step 3: Export to CSV
output_fp = '/Users/danielluna/Desktop/median_income_with_codes.csv'
median_income_df.to_csv(output_fp, index=False)
print(f"Saved updated CSV to {output_fp}")
print("Median Income Columns:\n", median_income_df.columns, "\n")

# Inner join on Code -> CDTA2020
merged_df = median_income_df.merge(
    cdta_df,
    left_on='Code',
    right_on='CDTA2020',
    how='left'
)

# Sort the DataFrame by 'Code'
merged_df = merged_df.sort_values(by='Code').reset_index(drop=True)

merged_df = merged_df[['Location','All Households','Families','Families with Children','Families without Children','Code','the_geom']]

# Step 1: Strip $ and commas from income columns and convert to numeric
income_cols = ['All Households', 'Families', 'Families with Children', 'Families without Children']
for col in income_cols:
    merged_df[col] = merged_df[col].replace('[\$,]', '', regex=True).astype(float)

# Step 2: Ensure string columns
string_cols = ['Location', 'Code', 'the_geom']
for col in string_cols:
    merged_df[col] = merged_df[col].astype(str)

# Quick check
print(merged_df.head())
print(merged_df.columns)
# Get the size of the dataframe
rows, cols = merged_df.shape
print(f"Merged DataFrame size: {rows} rows x {cols} columns")

# Optional: export to CSV
output_fp = '/Users/danielluna/Desktop/median_income_cdta_merged.csv'
merged_df.to_csv(output_fp, index=False)
print(f"Saved merged CSV to {output_fp}")


import pandas as pd
import os

DATA_DIR = os.path.join("..", "data", "raw")


# --- File Paths ---
# NOTE: Please ensure you have these two files in the same directory as the script.
# --- Load Data ---
try:
    income_df = pd.read_csv(os.path.join(DATA_DIR, "Household_Income.csv"))

    cdta_fp = pd.read_csv(os.path.join(DATA_DIR, "2020_Community_District_Tabulation.csv"))

    print("Files loaded successfully.")
except FileNotFoundError as e:
    print(f"Error loading files: {e}")
    print("Please make sure the correct filenames are used and the files are in the right directory.")
    # Exit if files cannot be loaded.
    exit()


# --- Data Preparation ---

# 1. Prepare the income data
# The 'Fips' column seems to encode borough and district. We'll convert it to a CDTA code.
# For example, 317 -> BK17 (Brooklyn Community District 17)

# Mapping from the first digit of 'Fips' to the borough prefix
boro_map = {
    1: 'MN',  # Manhattan
    2: 'BX',  # Bronx
    3: 'BK',  # Brooklyn
    4: 'QN',  # Queens
    5: 'SI'   # Staten Island
}

def create_cdta_code(fips):
    """Converts a FIPS-like number (e.g., 317) to a CDTA code (e.g., 'BK17')."""
    if pd.isna(fips):
        return None
    fips = int(fips)
    boro_digit = fips // 100
    district_num = fips % 100

    if boro_digit not in boro_map:
        return None

    boro_prefix = boro_map[boro_digit]
    # Format number with leading zero if needed (e.g., 1 -> '01')
    return f'{boro_prefix}{district_num:02d}'

# Create the new join key in the income dataframe
income_df['CDTACode'] = income_df['Fips'].apply(create_cdta_code)
print("\nCreated 'CDTACode' in the income dataframe. Head:")
print(income_df[['Location', 'Fips', 'CDTACode']].head())


# 2. Merge the two dataframes
# We merge income data with geographic data on the new CDTA code.
merged_df = income_df.merge(
    cdta_df,
    left_on='CDTACode',
    right_on='CDTA2020',
    how='left'  # Use a left join to keep all income data
)

# Check for rows that didn't merge
unmerged_count = merged_df['the_geom'].isna().sum()
if unmerged_count > 0:
    print(f"\nWarning: {unmerged_count} rows from the income data did not find a matching geography.")

# --- Filtering ---

# 3. Filter for Bronx and Brooklyn
# We use the 'BoroName' column from the geography data to select the desired boroughs.
# We also drop any rows that didn't have a matching geometry.
bronx_brooklyn_df = merged_df[merged_df['BoroName'].isin(['Brooklyn', 'Bronx'])].dropna(subset=['the_geom']).copy()
print(f"\nFiltered data to include only Brooklyn and Bronx. Resulting shape: {bronx_brooklyn_df.shape}")


# --- Final Cleanup & Export ---

# 4. Clean up and select final columns for Kepler.gl
final_cols = [
    'Location', 'Income Level', 'TimeFrame', 'Data', 'DataFormat',
    'CDTACode', 'BoroName', 'the_geom'
]
# Filter for columns that actually exist in the dataframe
final_cols_exist = [col for col in final_cols if col in bronx_brooklyn_df.columns]
final_df = bronx_brooklyn_df[final_cols_exist]


# 5. Save the result to a new CSV file
output_fp = 'bronx_and_brooklyn_household_income.csv'
final_df.to_csv(output_fp, index=False)

print(f"\nSuccessfully saved the filtered data to '{output_fp}'")
print("\nFinal data preview:")
print(final_df.head())


# --- Load Data ---
try:
    poverty_df = pd.read_csv(os.path.join(DATA_DIR, "Poverty.csv"))

    cdta_fp = pd.read_csv(os.path.join(DATA_DIR, "2020_Community_District_Tabulation.csv"))

    print("Files loaded successfully.")
except FileNotFoundError as e:
    print(f"Error loading files: {e}")
    print("Please make sure the correct filenames are used and the files are in the right directory.")
    # Exit if files cannot be loaded.
    exit()


# --- Data Preparation ---

# 1. Prepare the poverty data
# Convert the 'Fips' column into a standard CDTA code (e.g., 317 -> BK17).
boro_map = {
    1: 'MN',  # Manhattan
    2: 'BX',  # Bronx
    3: 'BK',  # Brooklyn
    4: 'QN',  # Queens
    5: 'SI'   # Staten Island
}

def create_cdta_code(fips):
    """Converts a FIPS-like number (e.g., 317) to a CDTA code (e.g., 'BK17')."""
    if pd.isna(fips):
        return None
    fips = int(fips)
    boro_digit = fips // 100
    district_num = fips % 100

    if boro_digit not in boro_map:
        return None

    boro_prefix = boro_map[boro_digit]
    # Format number with a leading zero if needed (e.g., 1 -> '01')
    return f'{boro_prefix}{district_num:02d}'

# Create the new join key in the poverty dataframe
poverty_df['CDTACode'] = poverty_df['Fips'].apply(create_cdta_code)
print("\nCreated 'CDTACode' in the poverty dataframe. Head:")
print(poverty_df[['Location', 'Fips', 'CDTACode']].head())


# 2. Merge the poverty and geography dataframes
merged_df = poverty_df.merge(
    cdta_df,
    left_on='CDTACode',
    right_on='CDTA2020',
    how='left'
)

# Check for rows that didn't merge
unmerged_count = merged_df['the_geom'].isna().sum()
if unmerged_count > 0:
    print(f"\nWarning: {unmerged_count} rows from the poverty data did not find a matching geography.")


# --- Filtering ---

# 3. Filter for Bronx and Brooklyn
# We use 'BoroName' from the geography data and drop any rows without a geometry.
bronx_brooklyn_df = merged_df[merged_df['BoroName'].isin(['Brooklyn', 'Bronx'])].dropna(subset=['the_geom']).copy()
print(f"\nFiltered data to include only Brooklyn and Bronx. Resulting shape: {bronx_brooklyn_df.shape}")


# --- Final Cleanup & Export ---

# 4. Select and clean up the final columns for Kepler.gl
final_cols = [
    'Location', 'TimeFrame', 'DataFormat', 'Data',
    'CDTACode', 'BoroName', 'the_geom'
]
# Ensure all selected columns exist in the DataFrame
final_cols_exist = [col for col in final_cols if col in bronx_brooklyn_df.columns]
final_df = bronx_brooklyn_df[final_cols_exist]


# 5. Save the result to a new CSV file
output_fp = 'bronx_and_brooklyn_poverty_data.csv'
final_df.to_csv(output_fp, index=False)

print(f"\nSuccessfully saved the filtered data to '{output_fp}'")
print("\nFinal data preview:")
print(final_df.head())



